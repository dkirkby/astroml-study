{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Theoretical Foundations, Continued</h1>\n",
    "<h1>Chapter 4:  Classical Statistical Inference (4.1-4.4)</h1>\n",
    "<hr/>\n",
    "\n",
    "<h2>4.2 Maximum Likelihood Estimation (MLE)</h2>\n",
    "\n",
    "<h3>4.2.1-4.2.2 The Likelihood Function</h3>\n",
    "\n",
    "What's the idea?  A set of data is a sample drawn from some distribution.  As such, each datum has a probabilty; if we make the assumption that these probabilities are independent, we arrive fairly intuitively at the measure of likelihood for the sample:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|M(\\vec{\\theta})) = {\\displaystyle \\prod_{i=1}^{n}} p(x_i|M(\\vec{\\theta})) \\tag{4.1}$$\n",
    "\n",
    "where $M$ is the model (the distribution the data is drawn from), and $\\vec{\\theta}$ are the parameters the model takes.\n",
    "\n",
    "Note:\n",
    "<list>\n",
    "<li>$L$ is not a PDF as it is not normalized</li>\n",
    "<li>In fact, commonly $L\\ll 1$, which leads to the use of Log-likelihood, $ln(L)$\n",
    "<li>$L$ can be considered both as a function of the model/distribution parameters, with fixed $\\{x_i\\}$, the case when trying to maximize it; or as a function of $x$, with fixed model parameters, when calculating the likelihood of some value.\n",
    "</list>\n",
    "\n",
    "\"All\" that needs now be done is take the derivative $L$ (or $ln(L)$), set to zero, and solve for the parameters giving the maximum.  Once this is done, confidence estimates for the parameters can be determined, either analytically or (more likely) numerically.  Finally, hypothesis tests/goodness of fit must be determined.\n",
    "\n",
    "<h3>4.2.4 Properties of Maximum Likelihood Estimators</h3>\n",
    "\n",
    "Assuming that the model $M$ truly is the correct class of distribution from which data ${x_i}$ are drawn, MLE's have several optimality properties.\n",
    "<list>\n",
    "<li>They are consistent, converging to the true value as data points increase</li>\n",
    "<li>They are asymptotically normal:  the distibution of the parameter estimate approaches a normal distribution about the MLE as data points increase; the spread($\\sigma$) of this distribution can be used as a confidence interval about the estimate.</li>\n",
    "<li>They achieve the minimum possible variance given the data at hand</li>\n",
    "</list>\n",
    "\n",
    "<h3>4.2.3 The Homoscedastic Gaussian Likelihood</h3>\n",
    "\n",
    "Given $N$ measurements, $\\{x_i\\}$, with a known, identical, normal error $\\sigma$ the likelihood function becomes:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = {\\displaystyle \\prod_{i=1}^{N}} \\frac{1}{\\sqrt{2\\pi\\sigma}} exp\\left(\\frac{-(x_i - \\mu)^2}{2\\sigma^2}\\right) \\tag{4.2}$$\n",
    "\n",
    "with only one parameter, $\\vec{\\theta}=(\\mu)$, or simply $\\theta=\\mu$.\n",
    "\n",
    "Using the log-likelihood here is doubly useful; besides rendering tiny numbers more numerical/computationally managable, here, analytically, it turns multiplications into additions, and those additions are logs of exponentials, so that:\n",
    "\n",
    "$$ln(L)={\\displaystyle \\sum_{i=1}^{N}} ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma}} exp\\left(\\frac{-(x_i - \\mu)^2}{2\\sigma^2}\\right)\\right) = {\\displaystyle \\sum_{i=1}^{N}}\\left( ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right) + \\frac{-(x_i - \\mu)^2}{2\\sigma^2}\\right) \\tag{D1}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$ln(L(\\mu))=constant-{\\displaystyle \\sum_{i=1}^{N}} \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\tag{4.4}$$\n",
    "\n",
    "Setting the derivative (by the only parameter, $\\mu$) equal to zero gives:\n",
    "\n",
    "$$\\frac{d~ln(L(\\mu))}{d~\\mu}=-{\\displaystyle \\sum_{i=1}^{N}} \\frac{-2(x_i - \\mu)}{2\\sigma^2}=0 \\implies {\\displaystyle \\sum_{i=1}^{N}} (x_i - \\mu) = 0 \\tag{D2}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\mu_{mle} = \\frac{1}{N}{\\displaystyle \\sum_{i=1}^{N}}x_i \\tag{4.5}$$\n",
    "\n",
    "As expected, as it should be.\n",
    "\n",
    "<h3>4.2.6 The Heteroscedastic Gaussian Likelihood</h3>\n",
    "\n",
    "Rather than the case of equation (4.4), we now have different errors per datum, $\\sigma_i$:\n",
    "\n",
    "$$ln(L(\\mu))=constant-{\\displaystyle \\sum_{i=1}^{N}} \\frac{(x_i - \\mu)^2}{2\\sigma_i^2} \\tag{4.8}$$\n",
    "\n",
    "from which, with $w_i = \\sigma_i^{-2}$, and following (D2) above:\n",
    "\n",
    "$$\\mu_{mle} = \\frac{\\displaystyle \\sum_{i=1}^{N}w_i x_i}{\\displaystyle \\sum_{i=1}^{N}w_i} \\tag{4.5}$$\n",
    "\n",
    "aka, simply the weighted mean.\n",
    "\n",
    "<h3>4.2.5 MLE Confidence Intervals</h3>\n",
    "\n",
    "Given a maximum likelihood estimate of e.g. $\\mu_{mle}$ as above, what is its uncertainty?\n",
    "\n",
    "$$\\sigma_{jk} = \\left( - \\frac{d^2 ~ln(L(\\theta))}{d~\\theta_j d\\theta_k} \\right)^{-1/2} \\tag{4.6}$$\n",
    "\n",
    "for $\\theta=\\vec{\\theta}_{mle}$.  For why this is, the text refers the reader to the Wasserman textbooks (after a brief description.)  Without the why: the diagnonal elements $\\sigma_{ii}$ correspond to marginal errors for $\\theta_i$, while the $\\sigma_{ij}$ with $i \\neq j$ indicate correlation of errors for $\\theta_i$ and $\\theta_j$.\n",
    "\n",
    "For the Guassian cases,\n",
    "\n",
    "$$\\sigma_{\\mu} = \\left(- \\frac{d^2 ~ln(L(\\mu))}{d~\\mu^2} \\right)^{-1/2} = \\left({\\displaystyle \\sum_{i=1}^{N}\\frac{1}{\\sigma_i^2}}\\right)^{-1/2} \\tag{4.7/4.10}$$\n",
    "\n",
    "<h3>4.2.7 MLE in the Case of Truncated and Censored Data</h3>\n",
    "\n",
    "A variable to be measured, x, has some range of possible values; due to e.g. the measuring aparatus used, the range of values is not sampled uniformly.  So: $S(x) \\neq c$, where $S$ is the PDF for sampling x.  In particular, for some values of $x$, $S(x)=0$.  When this last is true for some $x<x_{min}$ and/or $x > x_{max}$, the data set is said to be truncated.  (Censored data is where data has been removed for some reason.)\n",
    "\n",
    "If we take the Gaussian case, and the simple truncation $$S(x) = \\left\\{\\begin{array}{ll}c & x_{min} \\leq x \\leq x_{max} \\\\ 0 & otherwise \\\\ \\end{array} \\right. $$ with $c$ a constant to make $S(x)$ sum to 1.\n",
    "\n",
    "The probability distribution for $x$ needs to be renormalized to account for this truncation:  $p(x)$ needs to be scaled to become $C~p(x)$ such that $1 = {\\displaystyle \\int_{-\\infty}^{\\infty}} C~p(x)$  For this example case this is simple:\n",
    "\n",
    "$$C = C(\\mu, \\sigma, x_{min}, x_{max}) = \\frac{1}{P(x_{max}|\\mu, \\sigma) - P(x_{min}|\\mu, \\sigma)} \\tag{4.12}$$\n",
    "\n",
    "leading to a log-likelihood of:\n",
    "\n",
    "$$ln(L(\\mu))=constant -{\\displaystyle \\sum_{i=1}^{N}} \\frac{(x_i - \\mu)^2}{2\\sigma^2} + N~ln(C(\\mu, \\sigma, x_{min}, x_{max})) \\tag{4.13}$$\n",
    "\n",
    "<h3>4.2.8 Beyond the Likelihood:  Other Cost Functions and Robustness</h3>\n",
    "\n",
    "MLE represents the most common choice of cost functions.  The expectation value of the cost function is called \"risk.\"  Minimizing risk is a way to obtain best-fit parameters.\n",
    "\n",
    "The mean integrated square error (MISE),\n",
    "\n",
    "$$MISE = \\displaystyle \\int_{-\\infty}^{\\infty} [f(x) - h(x)]^2 d~x \\tag{4.14}$$\n",
    "\n",
    "is often used.  MISE is based on Mean Squared Error (MSE), aka the $L_2$ norm.  A cost function minimizing the absoluate deviation is called the $L_1$ norm.  Many cost functions, with different properties; a particularly useful example of a property is robustness to outliers.\n",
    "\n",
    "In chapters 6-10 cost functions will be important for various methods; this is particularly true when formalizing the likelihood function is difficult, because an optimal solultion can still eb found by minimizing the risk.\n",
    "\n",
    "<h2>4.3 The Goodness of Fit and Model Selection</h2>\n",
    "\n",
    "MLE estimates the best-fit parameters and gives us their uncertainties, but does not tell us how good a fit the model/parameters are.  What if a Gaussian model was choosen by e.g. the truth was Laplacian?  And what if a polynomial is being fit:  a higher order polynomial will always fit data better than a lower order polynomial, but is the higher order polynomial a better fit to the underlying process (e.g., are we just fitting noise or actually fitting additional complexity in the underlying distribution/process?)\n",
    "\n",
    "<h3>4.3.1 The Goodness of Fit for a Model</h3>\n",
    "\n",
    "In the Gaussian case, we have (4.4):\n",
    "\n",
    "$$ln(L(\\mu))=constant-{\\displaystyle \\sum_{i=1}^{N}} \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\tag{4.4}$$\n",
    "\n",
    "which may be re-written with $z_i=(x_i - \\mu)/\\sigma$ as\n",
    "\n",
    "$$ln(L(\\mu))=constant-{\\displaystyle \\sum_{i=1}^{N}} z_i^2 = constant - \\frac{1}{2}\\chi^2 \\tag{4.15}$$\n",
    "\n",
    "and hence the distibution of $ln(L)$ can be determined from the $\\chi^2$ distribution with $N-k$ degrees of freedom, with $k$ model parameters.  With an expectation value of $N-k$, for a \"good fit\" we should have $\\chi_{dof}^2=\\frac{\\chi^2}{N-k} \\approx 1$  (As in chapter 3, the warning here is that $\\chi$ is very sensitive to outliers.)\n",
    "\n",
    "The probability that a certain ML value $L_{mle}$ arose by chance can only be evaluated by $\\chi^2$ when the likelihood is Gaussian; otherwise $L_{mle}$ is still a measure of how well a model fits the data.  Asssuming the same $k$, models can be ranked by their likelihood.  But the $L_{mle}$ value(s) alone do not indicated in an <i>absolute</i> sense how well the model(s) fit the data; to know that requires knowing the distribution of $L_{mle}$, as given by $\\chi^2$ for a Gaussian likelihood.\n",
    "\n",
    "<h3>4.3.2 Model Comparison</h3>\n",
    "\n",
    "The best way to compare models is cross-validation, but this topic is covered in detail in later chapters.\n",
    "\n",
    "The Aikake Information Criterion (AIC) is a simple method for comparing models that (attempts to) accounts for model complexity in addition to $L_{mle}$ when comparing models.  AIC is defined as:\n",
    "\n",
    "$$AIC \\equiv -2~ln(L_{mle}) + 2k + \\frac{2k(k+1)}{N-k-1} \\tag{4.17}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$AIC = \\frac{2~k~N}{N-(k+1)} - 2~ln(L_{mle})$$\n",
    "\n",
    "Out of multiple possible models, the one with the smallest AIC is the best.\n",
    "\n",
    "<h2>4.4 ML Applied to Gaussian Mixtures:  The Expectation Maximization Algorithm</h2>\n",
    "\n",
    "A special case of a complex likelihood which can still be maximized simply (and treated analytically) is a mixture of Gaussians.\n",
    "\n",
    "<h3>4.4.1 Gaussian Mixture Model</h3>\n",
    "\n",
    "For a model made up of $M$ Gaussians the likelihood of a given datum $x_i$ is:\n",
    "\n",
    "$$p(x_i|\\vec{\\theta}) = {\\displaystyle \\sum_{i=1}^{M} \\alpha_j ~ \\mathcal{N}(\\mu_j, \\sigma_j)} \\tag{4.18}$$\n",
    "\n",
    "where, because we require each point to be drawn from a true pdf, the normalization constants $\\alpha_j$ must sum to 1.  The log-likelihood is then:\n",
    "\n",
    "$$ln(L)={\\displaystyle \\sum_{i=1}^{N} ln \\left( {\\displaystyle \\sum_{i=1}^{M} \\alpha_j ~ \\mathcal{N}(\\mu_j, \\sigma_j)} \\right)} \\tag{4.20}$$\n",
    "\n",
    "with $k=3M-1$ parameters.\n",
    "\n",
    "<h3>Class Labels and Hidden Variables</h3>\n",
    "\n",
    "A variety of more advanced methods are available for maximizing $ln(L)$, but a fast and and realatively easy method is \"hidden variables\".  Each of the $M$ Guassians above are interpreted as a class such that any individual $x_i$ was generated by one and only one Gaussian.  The hidden variable is $j$, identifying which class each $x_i$ belongs to.  If each point's class is known, the problem resolves to $M$ separate MLE problems with Guassian models, as developed so far.  The fraction of points in each class would be an estimator for the corresponding normalization factor, $\\alpha_j$.  When the class labels are known but the underlying distribution is not Gaussian, the \"naive Bayesian classfier\" ($\\S$ 9.3.2) can be used.\n",
    "\n",
    "Continuing with the Gaussian case here, using Bayes' rule we find the probability of a given class for a given $x_i$:\n",
    "\n",
    "$$p(j|x_i)=\\frac{\\alpha_j ~ \\mathcal{N}(\\mu_j,\\sigma_j)}{\\displaystyle \\sum_{j=1}^{M} \\alpha_j ~ \\mathcal{N}(\\mu_j, \\sigma_j)} \\tag{4.21}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$p(j|x_i) = \\frac{\\alpha_j ~ p(x_i|\\mu_j,\\sigma_j)}{\\displaystyle \\sum_{j=1}^{M} \\alpha_j~p(x_i|\\mu_j, \\sigma_j)} = \\frac{p(j) ~ p(x_i|\\mu_j,\\sigma_j)}{p(x_i)}\n",
    "\\tag{D3}$$\n",
    "\n",
    "How to use (4.21) and (4.20) to come up with a way to handle this?\n",
    "\n",
    "<h3>4.4.3 The Basics of the Expectation Maximization Algorithm</h3>\n",
    "\n",
    "Replacing $\\mathcal{N}(\\mu_j, \\sigma_j)$ with the general $p_j(x_i|\\vec{\\theta})$ in (4.20) and taking the partial derivative with respect to $\\theta_j$, then rearranging gives:\n",
    "\n",
    "$$\\frac{\\partial~ln(L)}{\\partial~\\theta_j} = {\\displaystyle \\sum_{i=1}^{N} \\left( \\frac{\\alpha_j~p_j(x_i|\\vec{\\theta})}{\\displaystyle \\sum_{j=1}^{M} \\alpha_j~p(x_i|\\vec{\\theta})} \\right)} \\left( \\frac{1}{p_j(x_i|\\vec{\\theta})} \\frac{\\partial~p_j(x_i|\\vec{\\theta})}{\\partial~\\theta_j} \\right) \\tag{4.24}$$\n",
    "\n",
    "where the first part is just (4.21/D3).  For the EM algorithm we assume this is fixed during each iteration; this whole term is then replaced with $w_{ij}$.  The second part is just a partial of the $ln(p_j(x_i|\\vec{\\theta}))$ and, when Gaussian as in our work so far, gives:\n",
    "\n",
    "$$\\frac{\\partial~ln(L)}{\\partial~\\theta_j} = -{\\displaystyle \\sum_{i=1}^{N}} w_{ij} \\frac{\\partial}{\\partial~\\theta_j} \\left( ln(\\sigma_j) + \\frac{(x_i - \\mu_j)^2}{2~\\sigma_j^2} \\right)$$\n",
    "\n",
    "and leads to the estimators for $\\mu_j$, $\\sigma_j$, and $\\alpha_j$:\n",
    "\n",
    "$$\\mu_j = \\frac{\\displaystyle \\sum_{i=1}^{N} w_{ij} x_i}{\\displaystyle \\sum_{i=1}^{N} w_{ij}} \\tag{4.26}$$\n",
    "\n",
    "$$\\sigma_j^2 = \\frac{\\displaystyle \\sum_{i=1}^{N} w_{ij}(x_i-\\mu_j)^2}{\\displaystyle \\sum_{i=1}^{N} w_{ij}} \\tag{4.27}$$\n",
    "\n",
    "and (somewhat circularly)\n",
    "\n",
    "$$\\alpha_j = \\frac{1}{N}{\\displaystyle \\sum_{i=1}^{N} w_{ij}} \\tag{4.28}$$\n",
    "\n",
    "The EM algorithm starts with a guess for $w_{ij}$, then the maximization step (M-step) of evaluating (4.26-4.28), then a expectation step (E-step) of updating $w_{ij}$ based on the M-step outputs.  The M-step and E-step are run iteratively until convergence, iteration limit, etc.\n",
    "\n",
    "Similar to overfitting with e.g. a high-degree polynomial, setting M too high will e.g. split data that should be classed together.  Choosing the appropriate M is a case of model selection, and AIC (or BIC, later) should be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
