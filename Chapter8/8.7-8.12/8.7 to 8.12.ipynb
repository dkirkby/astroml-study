{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.7 to 8.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Nonlinear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forcing data to linear models is a very common technique in astronomy but it can lead to other complications making nonlinear regression useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Cosmological parameter inference from supernova (SN) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the probability of a certain set of cosmological parameters for a $\\Lambda$CDM model:\n",
    "\n",
    "$p(\\Omega_{m}, \\Omega_{\\Lambda} | z, I) \\propto \\prod_{i=1}^{n}\\cfrac{1}{\\sqrt{2\\pi}\\sigma_{i}} \\exp\\left(-\\cfrac{(\\mu_{i} -  \\mu (z_{i} | \\Omega_{m},\\Omega_{\\Lambda}))^{2}}{2\\sigma_{i}^{2}}\\right)p(\\Omega_{m},\\Omega_{\\Lambda})$\n",
    "\n",
    "We can find the posterior using a Markov Chain Monte Carlo (MCMC) or use a Levenberg-Marquardt (LM) algorithm to find the maximum likelihood.\n",
    "\n",
    "$\\mu_{i}$ distance modulus, $\\Omega_{m}, \\Omega_{\\Lambda}$ are cosmological parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.datasets import generate_mu_z\n",
    "from astroML.cosmology import Cosmology\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate the data\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, z0=0.3,\n",
    "                                         dmu_0=0.05, dmu_1=0.004,\n",
    "                                         random_state=1)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# define a log likelihood in terms of the parameters\n",
    "#  beta = [omegaM, omegaL]\n",
    "def compute_logL(beta):\n",
    "    cosmo = Cosmology(omegaM=beta[0], omegaL=beta[1])\n",
    "    mu_pred = np.array(map(cosmo.mu, z_sample))\n",
    "    return - np.sum(0.5 * ((mu_sample - mu_pred) / dmu) ** 2)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define a function to compute (and save to file) the log-likelihood\n",
    "@pickle_results('mu_z_nonlinear.pkl')\n",
    "def compute_mu_z_nonlinear(Nbins=50):\n",
    "    omegaM = np.linspace(0.05, 0.75, Nbins)\n",
    "    omegaL = np.linspace(0.4, 1.1, Nbins)\n",
    "\n",
    "    logL = np.empty((Nbins, Nbins))\n",
    "\n",
    "    for i in range(len(omegaM)):\n",
    "        #print '%i / %i' % (i + 1, len(omegaM))\n",
    "        for j in range(len(omegaL)):\n",
    "            logL[i, j] = compute_logL([omegaM[i], omegaL[j]])\n",
    "\n",
    "    return omegaM, omegaL, logL\n",
    "\n",
    "omegaM, omegaL, res = compute_mu_z_nonlinear()\n",
    "res -= np.max(res)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.25,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left plot: the data and best-fit\n",
    "ax = fig.add_subplot(121)\n",
    "whr = np.where(res == np.max(res))\n",
    "omegaM_best = omegaM[whr[0][0]]\n",
    "omegaL_best = omegaL[whr[1][0]]\n",
    "cosmo = Cosmology(omegaM=omegaM_best, omegaL=omegaL_best)\n",
    "\n",
    "z_fit = np.linspace(0.04, 2, 100)\n",
    "mu_fit = np.asarray(map(cosmo.mu, z_fit))\n",
    "\n",
    "ax.plot(z_fit, mu_fit, '-k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray')\n",
    "\n",
    "ax.set_xlim(0, 1.8)\n",
    "ax.set_ylim(36, 46)\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.text(0.04, 0.96, \"%i observations\" % len(z_sample),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "# right plot: the likelihood\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "ax.contour(omegaM, omegaL, convert_to_stdev(res.T),\n",
    "           levels=(0.683, 0.955, 0.997),\n",
    "           colors='k')\n",
    "\n",
    "ax.plot([0, 1], [1, 0], '--k')\n",
    "ax.plot([0, 1], [0.73, 0.73], ':k')\n",
    "ax.plot([0.27, 0.27], [0, 2], ':k')\n",
    "\n",
    "ax.set_xlim(0.05, 0.75)\n",
    "ax.set_ylim(0.4, 1.1)\n",
    "\n",
    "ax.set_xlabel(r'$\\Omega_M$')\n",
    "ax.set_ylabel(r'$\\Omega_\\Lambda$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Uncertainties in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most problems the assumption of having one error-free variable is not valid (usually the independent variable $x$). Errors in $x$ translate into bias in the derived regression coefficients.\n",
    "\n",
    "Assuming Gaussian errors the covariance matrix can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Sigma_{i} = \\left[ \\begin{array}{cc}\n",
    "\\sigma^{2}_{x_{i}} & \\sigma{xy_{i}}\\\\\n",
    "\\sigma_{xy_{i}} & \\sigma^{2}{y_{i}}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for a straight line regression we can write the slope in terms of the normal vector $\\mathbf{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{n}=\\left[\\begin{array}{c}\n",
    "                 -\\sin{\\alpha}\\\\\n",
    "                 \\cos{\\alpha}\n",
    "                \\end{array}\n",
    "                \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection of the covariance matrix is just:\n",
    "\n",
    "$$S_{i}^{2}=\\mathbf{n}^{T}\\Sigma_{i}\\mathbf{n}$$\n",
    "\n",
    "and the distance between a point and the line is given by:\n",
    "$$\\Delta_{i} = \\mathbf{n}^{T} z_{i} - \\theta_{0}\\cos{\\alpha}$$\n",
    "where $z_{i}$ represents the data point $(x_{i},y_{i})$. The log-likelihood is then\n",
    "$$\\ln{L} = -\\sum_{i}\\frac{\\Delta_{i}^{2}}{2S_{i}^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from astroML.linear_model import TLS_logL\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define some convenience functions\n",
    "\n",
    "# translate between typical slope-intercept representation,\n",
    "# and the normal vector representation\n",
    "def get_m_b(beta):\n",
    "    b = np.dot(beta, beta) / beta[1]\n",
    "    m = -beta[0] / beta[1]\n",
    "    return m, b\n",
    "\n",
    "\n",
    "def get_beta(m, b):\n",
    "    denom = (1 + m * m)\n",
    "    return np.array([-b * m / denom, b / denom])\n",
    "\n",
    "\n",
    "# compute the ellipse pricipal axes and rotation from covariance\n",
    "def get_principal(sigma_x, sigma_y, rho_xy):\n",
    "    sigma_xy2 = rho_xy * sigma_x * sigma_y\n",
    "\n",
    "    alpha = 0.5 * np.arctan2(2 * sigma_xy2,\n",
    "                             (sigma_x ** 2 - sigma_y ** 2))\n",
    "    tmp1 = 0.5 * (sigma_x ** 2 + sigma_y ** 2)\n",
    "    tmp2 = np.sqrt(0.25 * (sigma_x ** 2 - sigma_y ** 2) ** 2 + sigma_xy2 ** 2)\n",
    "\n",
    "    return np.sqrt(tmp1 + tmp2), np.sqrt(tmp1 - tmp2), alpha\n",
    "\n",
    "\n",
    "# plot ellipses\n",
    "def plot_ellipses(x, y, sigma_x, sigma_y, rho_xy, factor=2, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    sigma1, sigma2, alpha = get_principal(sigma_x, sigma_y, rho_xy)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        ax.add_patch(Ellipse((x[i], y[i]),\n",
    "                             factor * sigma1[i], factor * sigma2[i],\n",
    "                             alpha[i] * 180. / np.pi,\n",
    "                             fc='none', ec='k'))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# We'll use the data from table 1 of Hogg et al. 2010\n",
    "data = fetch_hogg2010test()\n",
    "data = data[5:]  # no outliers\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "sigma_x = data['sigma_x']\n",
    "sigma_y = data['sigma_y']\n",
    "rho_xy = data['rho_xy']\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Find best-fit parameters\n",
    "X = np.vstack((x, y)).T\n",
    "dX = np.zeros((len(x), 2, 2))\n",
    "dX[:, 0, 0] = sigma_x ** 2\n",
    "dX[:, 1, 1] = sigma_y ** 2\n",
    "dX[:, 0, 1] = dX[:, 1, 0] = rho_xy * sigma_x * sigma_y\n",
    "\n",
    "min_func = lambda beta: -TLS_logL(beta, X, dX)\n",
    "beta_fit = optimize.fmin(min_func,\n",
    "                         x0=[-1, 1])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the data and fits\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.25,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# first let's visualize the data\n",
    "ax = fig.add_subplot(121)\n",
    "ax.scatter(x, y, c='k', s=9)\n",
    "plot_ellipses(x, y, sigma_x, sigma_y, rho_xy, ax=ax)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the best-fit line\n",
    "m_fit, b_fit = get_m_b(beta_fit)\n",
    "x_fit = np.linspace(0, 300, 10)\n",
    "ax.plot(x_fit, m_fit * x_fit + b_fit, '-k')\n",
    "\n",
    "ax.set_xlim(40, 250)\n",
    "ax.set_ylim(100, 600)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the likelihood contour in m, b\n",
    "ax = fig.add_subplot(122)\n",
    "m = np.linspace(1.7, 2.8, 100)\n",
    "b = np.linspace(-60, 110, 100)\n",
    "logL = np.zeros((len(m), len(b)))\n",
    "\n",
    "for i in range(len(m)):\n",
    "    for j in range(len(b)):\n",
    "        logL[i, j] = TLS_logL(get_beta(m[i], b[j]), X, dX)\n",
    "\n",
    "ax.contour(m, b, convert_to_stdev(logL.T),\n",
    "           levels=(0.683, 0.955, 0.997),\n",
    "           colors='k')\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('intercept')\n",
    "ax.set_xlim(1.7, 2.8)\n",
    "ax.set_ylim(-60, 110)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.9 Regression that is robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life you can perform incorrect measurements and your regression model fitting must be able to account for outliers from the fit. For standard least squares $L_{2}$ outliers can have a big leverage. If we knew the error distribution for all of our data points we could use that when defining the likelihood. Some useful techniques to avoid outliers is to employ $L_{1}$ norm or adopt an approach to reject outliers (usually called sigma clipping).\n",
    "\n",
    "M estimators modify the likelihood to be less sensistive to outliers substituting the standard least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huber loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Huber estimator minimizes\n",
    "\n",
    "$$\\Sigma_{i=1}^{N}e(y_{i}|y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $e(y_{i}|y)$ is modeled as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi(t) =\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "1/2 t^{2} & \\textrm{if } |t| \\leq c,\\\\\n",
    "c|t|-1/2c^{2} & \\textrm{if } |t| \\geq c,\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It assumes Gaussian behavior for errors close to the true value and exponential for large excursions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy import optimize\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data: this includes outliers\n",
    "data = fetch_hogg2010test()\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "dy = data['sigma_y']\n",
    "\n",
    "\n",
    "# Define the standard squared-loss function\n",
    "def squared_loss(m, b, x, y, dy):\n",
    "    y_fit = m * x + b\n",
    "    return np.sum(((y - y_fit) / dy) ** 2, -1)\n",
    "\n",
    "\n",
    "# Define the log-likelihood via the Huber loss function\n",
    "def huber_loss(m, b, x, y, dy, c=2):\n",
    "    y_fit = m * x + b\n",
    "    t = abs((y - y_fit) / dy)\n",
    "    flag = t > c\n",
    "    return np.sum((~flag) * (0.5 * t ** 2) - (flag) * c * (0.5 * c - t), -1)\n",
    "\n",
    "f_squared = lambda beta: squared_loss(beta[0], beta[1], x=x, y=y, dy=dy)\n",
    "f_huber = lambda beta: huber_loss(beta[0], beta[1], x=x, y=y, dy=dy, c=1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute the maximum likelihood using the huber loss\n",
    "beta0 = (2, 30)\n",
    "beta_squared = optimize.fmin(f_squared, beta0)\n",
    "beta_huber = optimize.fmin(f_huber, beta0)\n",
    "\n",
    "print beta_squared\n",
    "print beta_huber\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x_fit = np.linspace(0, 350, 10)\n",
    "ax.plot(x_fit, beta_squared[0] * x_fit + beta_squared[1], '--k',\n",
    "        label=\"squared loss:\\n $y=%.2fx + %.1f$\" % tuple(beta_squared))\n",
    "ax.plot(x_fit, beta_huber[0] * x_fit + beta_huber[1], '-k',\n",
    "        label=\"Huber loss:\\n $y=%.2fx + %.1f$\" % tuple(beta_huber))\n",
    "ax.legend(loc=4)\n",
    "\n",
    "ax.errorbar(x, y, dy, fmt='.k', lw=1, ecolor='gray')\n",
    "\n",
    "ax.set_xlim(0, 350)\n",
    "ax.set_ylim(100, 700)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian outlier methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Enhance the model to include naturally the presence of outliers:\n",
    "\n",
    "* Mixture model: Adds a background Gaussian component to the data (includes 3 additional parameters $\\mu_{b}$, $V_{b}$ and $p_{b}$ (mean and variance of the background and probability that any point is an outlier) $\\rightarrow$ Strategy: Marginalize over these parameters to obtain the robust fit.\n",
    "\n",
    "* Try to identify bad points individually and give a value $g_{i}=0$ for bad points and $g_{i}=1$ for good points. Use similar approach to mixture model adding a background Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "# Hack to fix import issue in older versions of pymc\n",
    "import scipy\n",
    "import scipy.misc\n",
    "scipy.derivative = scipy.misc.derivative\n",
    "import pymc\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data: this includes outliers\n",
    "data = fetch_hogg2010test()\n",
    "xi = data['x']\n",
    "yi = data['y']\n",
    "dyi = data['sigma_y']\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# First model: no outlier correction\n",
    "# define priors on beta = (slope, intercept)\n",
    "@pymc.stochastic\n",
    "def beta_M0(value=np.array([2., 100.])):\n",
    "    \"\"\"Slope and intercept parameters for a straight line.\n",
    "    The likelihood corresponds to the prior probability of the parameters.\"\"\"\n",
    "    slope, intercept = value\n",
    "    prob_intercept = 1 + 0 * intercept\n",
    "    # uniform prior on theta = arctan(slope)\n",
    "    # d[arctan(x)]/dx = 1 / (1 + x^2)\n",
    "    prob_slope = np.log(1. / (1. + slope ** 2))\n",
    "    return prob_intercept + prob_slope\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def model_M0(xi=xi, beta=beta_M0):\n",
    "    slope, intercept = beta\n",
    "    return slope * xi + intercept\n",
    "\n",
    "y = pymc.Normal('y', mu=model_M0, tau=dyi ** -2,\n",
    "                observed=True, value=yi)\n",
    "\n",
    "M0 = dict(beta_M0=beta_M0, model_M0=model_M0, y=y)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Second model: nuisance variables correcting for outliers\n",
    "# This is the mixture model given in equation 17 in Hogg et al\n",
    "\n",
    "# define priors on beta = (slope, intercept)\n",
    "@pymc.stochastic\n",
    "def beta_M1(value=np.array([2., 100.])):\n",
    "    \"\"\"Slope and intercept parameters for a straight line.\n",
    "    The likelihood corresponds to the prior probability of the parameters.\"\"\"\n",
    "    slope, intercept = value\n",
    "    prob_intercept = 1 + 0 * intercept\n",
    "    # uniform prior on theta = arctan(slope)\n",
    "    # d[arctan(x)]/dx = 1 / (1 + x^2)\n",
    "    prob_slope = np.log(1. / (1. + slope ** 2))\n",
    "    return prob_intercept + prob_slope\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def model_M1(xi=xi, beta=beta_M1):\n",
    "    slope, intercept = beta\n",
    "    return slope * xi + intercept\n",
    "\n",
    "# uniform prior on Pb, the fraction of bad points\n",
    "Pb = pymc.Uniform('Pb', 0, 1.0, value=0.1)\n",
    "\n",
    "# uniform prior on Yb, the centroid of the outlier distribution\n",
    "Yb = pymc.Uniform('Yb', -10000, 10000, value=0)\n",
    "\n",
    "# uniform prior on log(sigmab), the spread of the outlier distribution\n",
    "log_sigmab = pymc.Uniform('log_sigmab', -10, 10, value=5)\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def sigmab(log_sigmab=log_sigmab):\n",
    "    return np.exp(log_sigmab)\n",
    "\n",
    "\n",
    "# set up the expression for likelihood\n",
    "def mixture_likelihood(yi, model, dyi, Pb, Yb, sigmab):\n",
    "    \"\"\"Equation 17 of Hogg 2010\"\"\"\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    root2pi = np.sqrt(2 * np.pi)\n",
    "\n",
    "    L_in = (1. / root2pi / dyi\n",
    "            * np.exp(-0.5 * (yi - model) ** 2 / Vi))\n",
    "\n",
    "    L_out = (1. / root2pi / np.sqrt(Vi + Vb)\n",
    "             * np.exp(-0.5 * (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return np.sum(np.log((1 - Pb) * L_in + Pb * L_out))\n",
    "\n",
    "MixtureNormal = pymc.stochastic_from_dist('mixturenormal',\n",
    "                                          logp=mixture_likelihood,\n",
    "                                          dtype=np.float,\n",
    "                                          mv=True)\n",
    "\n",
    "y_mixture = MixtureNormal('y_mixture', model=model_M1, dyi=dyi,\n",
    "                          Pb=Pb, Yb=Yb, sigmab=sigmab,\n",
    "                          observed=True, value=yi)\n",
    "\n",
    "M1 = dict(y_mixture=y_mixture, beta_M1=beta_M1, model_M1=model_M1,\n",
    "          Pb=Pb, Yb=Yb, log_sigmab=log_sigmab, sigmab=sigmab)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Third model: marginalizes over the probability that each point is an outlier.\n",
    "# define priors on beta = (slope, intercept)\n",
    "@pymc.stochastic\n",
    "def beta_M2(value=np.array([2., 100.])):\n",
    "    \"\"\"Slope and intercept parameters for a straight line.\n",
    "    The likelihood corresponds to the prior probability of the parameters.\"\"\"\n",
    "    slope, intercept = value\n",
    "    prob_intercept = 1 + 0 * intercept\n",
    "    # uniform prior on theta = arctan(slope)\n",
    "    # d[arctan(x)]/dx = 1 / (1 + x^2)\n",
    "    prob_slope = np.log(1. / (1. + slope ** 2))\n",
    "    return prob_intercept + prob_slope\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def model_M2(xi=xi, beta=beta_M2):\n",
    "    slope, intercept = beta\n",
    "    return slope * xi + intercept\n",
    "\n",
    "# qi is bernoulli distributed\n",
    "# Note: this syntax requires pymc version 2.2\n",
    "qi = pymc.Bernoulli('qi', p=1 - Pb, value=np.ones(len(xi)))\n",
    "\n",
    "\n",
    "def outlier_likelihood(yi, mu, dyi, qi, Yb, sigmab):\n",
    "    \"\"\"likelihood for full outlier posterior\"\"\"\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    root2pi = np.sqrt(2 * np.pi)\n",
    "\n",
    "    logL_in = -0.5 * np.sum(qi * (np.log(2 * np.pi * Vi)\n",
    "                                  + (yi - mu) ** 2 / Vi))\n",
    "\n",
    "    logL_out = -0.5 * np.sum((1 - qi) * (np.log(2 * np.pi * (Vi + Vb))\n",
    "                                         + (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return logL_out + logL_in\n",
    "\n",
    "OutlierNormal = pymc.stochastic_from_dist('outliernormal',\n",
    "                                          logp=outlier_likelihood,\n",
    "                                          dtype=np.float,\n",
    "                                          mv=True)\n",
    "\n",
    "y_outlier = OutlierNormal('y_outlier', mu=model_M2, dyi=dyi,\n",
    "                          Yb=Yb, sigmab=sigmab, qi=qi,\n",
    "                          observed=True, value=yi)\n",
    "\n",
    "M2 = dict(y_outlier=y_outlier, beta_M2=beta_M2, model_M2=model_M2,\n",
    "          qi=qi, Pb=Pb, Yb=Yb, log_sigmab=log_sigmab, sigmab=sigmab)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the data\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.25,\n",
    "                    bottom=0.1, top=0.95, hspace=0.2)\n",
    "\n",
    "# first axes: plot the data\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.errorbar(xi, yi, dyi, fmt='.k', ecolor='gray', lw=1)\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Go through models; compute and plot likelihoods\n",
    "models = [M0, M1, M2]\n",
    "linestyles = [':', '--', '-']\n",
    "labels = ['no outlier correction\\n(dotted fit)',\n",
    "          'mixture model\\n(dashed fit)',\n",
    "          'outlier rejection\\n(solid fit)']\n",
    "\n",
    "\n",
    "x = np.linspace(0, 350, 10)\n",
    "\n",
    "bins = [(np.linspace(140, 300, 51), np.linspace(0.6, 1.6, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51))]\n",
    "\n",
    "for i, M in enumerate(models):\n",
    "    S = pymc.MCMC(M)\n",
    "    S.sample(iter=25000, burn=5000)\n",
    "    trace = S.trace('beta_M%i' % i)\n",
    "\n",
    "    H2D, bins1, bins2 = np.histogram2d(trace[:, 0], trace[:, 1], bins=50)\n",
    "    w = np.where(H2D == H2D.max())\n",
    "\n",
    "    # choose the maximum posterior slope and intercept\n",
    "    slope_best = bins1[w[0][0]]\n",
    "    intercept_best = bins2[w[1][0]]\n",
    "\n",
    "    # plot the best-fit line\n",
    "    ax1.plot(x, intercept_best + slope_best * x, linestyles[i], c='k')\n",
    "\n",
    "    # For the model which identifies bad points,\n",
    "    # plot circles around points identified as outliers.\n",
    "    if i == 2:\n",
    "        qi = S.trace('qi')[:]\n",
    "        Pi = qi.astype(float).mean(0)\n",
    "        outlier_x = xi[Pi < 0.32]\n",
    "        outlier_y = yi[Pi < 0.32]\n",
    "        ax1.scatter(outlier_x, outlier_y, lw=1, s=400, alpha=0.5,\n",
    "                    facecolors='none', edgecolors='red')\n",
    "\n",
    "    # plot the likelihood contours\n",
    "    ax = plt.subplot(222 + i)\n",
    "\n",
    "    H, xbins, ybins = np.histogram2d(trace[:, 1], trace[:, 0], bins=bins[i])\n",
    "    H[H == 0] = 1E-16\n",
    "    Nsigma = convert_to_stdev(np.log(H))\n",
    "\n",
    "    ax.contour(0.5 * (xbins[1:] + xbins[:-1]),\n",
    "               0.5 * (ybins[1:] + ybins[:-1]),\n",
    "               Nsigma.T, levels=[0.683, 0.955], colors='black')\n",
    "\n",
    "    ax.set_xlabel('intercept')\n",
    "    ax.set_ylabel('slope')\n",
    "    ax.grid(color='gray')\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(40))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "\n",
    "    ax.text(0.98, 0.98, labels[i], ha='right', va='top',\n",
    "            bbox=dict(fc='w', ec='none', alpha=0.5),\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_xlim(bins[i][0][0], bins[i][0][-1])\n",
    "    ax.set_ylim(bins[i][1][0], bins[i][1][-1])\n",
    "\n",
    "ax1.set_xlim(0, 350)\n",
    "ax1.set_ylim(100, 700)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.10 Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name it can be applied to data not generated by Gaussian process. It is a collection of random variables in parameter space, any subset of which is defined by a joint Gaussian distribution.\n",
    "\n",
    "* Similar idea to Fourier transform: We can write an infinite set of functions $f(x)$ from Gaussian kernels but this infinite space can be transformed to a finite covariance space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming exponential squared covariances\n",
    "\n",
    "$$\\textrm{Cov}{(x_{1},x_{2};h)} = \\exp{\\left(\\frac{\\left(x_{1}-x_{2}\\right)^{2}}{2h^{2}}\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the covariance matrix $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$K = \\left(\\begin{array}{cc}\n",
    "           K_{11} & K_{12}\\\\\n",
    "            K_{12}^{T} & K_{22}\n",
    "            \\end{array}\n",
    "           \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K_{11}$ covariance between input points $x_{i}$ with observational errors $\\sigma_{i}$ added in quadrature to the diagonal, $K_{12}$ cross-covariance between the input points $x_{i}$ and the unknown points $x_{j}^{*}$, $K_{22}$ covariance between the unknown points $x_{j}^{*}$. For the observed vectors $\\mathbf{x}$, $\\mathbf{y}$ and the unknown points $\\mathbf{x}^{*}$ the posterior is given by\n",
    "\n",
    "$$p(f_{j}|\\left\\{x_{i},y_{i},\\sigma_{i}\\right\\},x_{j}^{*}) = \\mathcal{N}(\\mu,\\Sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "$$\\mu = K_{12}K_{11}^{-1}\\mathbf{y}$$\n",
    "$$\\Sigma = K_{22} - K_{12}^{T}K_{11}^{-1}K_{12}$$\n",
    "\n",
    "Underlying physics enter in the form of the assumed covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate data\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "cosmo = Cosmology()\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = np.asarray(map(cosmo.mu, z))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit the data\n",
    "# Mesh the input space for evaluations of the real function,\n",
    "# the prediction and its MSE\n",
    "z_fit = np.linspace(0, 2, 1000)\n",
    "gp = GaussianProcess(corr='squared_exponential', theta0=1e-1,\n",
    "                     thetaL=1e-2, thetaU=1,\n",
    "                     normalize=False,\n",
    "                     nugget=(dmu / mu_sample) ** 2,\n",
    "                     random_start=1)\n",
    "gp.fit(z_sample[:, None], mu_sample)\n",
    "y_pred, MSE = gp.predict(z_fit[:, None], eval_MSE=True)\n",
    "sigma = np.sqrt(MSE)\n",
    "print gp.theta_\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the gaussian process\n",
    "#  gaussian process allows computation of the error at each point\n",
    "#  so we will show this as a shaded region\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.1, top=0.95)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_true, '--k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', markersize=6)\n",
    "ax.plot(z_fit, y_pred, '-k')\n",
    "ax.fill_between(z_fit, y_pred - 1.96 * sigma, y_pred + 1.96 * sigma,\n",
    "                alpha=0.2, color='b', label='95% confidence interval')\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(36, 48)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.11 Overfitting, Underfitting and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Underfitting: The model is not complex enough to describe the data $\\rightarrow$ You need more parameters in your model!\n",
    "\n",
    "* Overfitting: Your model is too complex for your data $\\rightarrow$ Maybe your fit goes through all your data points but you have a comparable number of parameters and data points.\n",
    "\n",
    "AIC or BIC are useful to deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation (one of A. Connolly's favorites!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide your sample into three: training, cross-validation, and test. Get your optimal model parameters from the training set and evaluate the errors on your cross-validation set. If the error it's large, your model is biased (overfit). The model that minimizes the cross-validation error is the best model in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Select the cross-validation points\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(20)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Third figure: plot errors as a function of polynomial degree d\n",
    "d = np.arange(0, 21)\n",
    "training_err = np.zeros(d.shape)\n",
    "crossval_err = np.zeros(d.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "for i in range(len(d)):\n",
    "    p = np.polyfit(x, y, d[i])\n",
    "    training_err[i] = np.sqrt(np.sum((np.polyval(p, x) - y) ** 2)\n",
    "                              / len(y))\n",
    "    crossval_err[i] = np.sqrt(np.sum((np.polyval(p, x_cv) - y_cv) ** 2)\n",
    "                              / len(y_cv))\n",
    "\n",
    "BIC_train = np.sqrt(len(y)) * training_err / dy + d * np.log(len(y))\n",
    "BIC_crossval = np.sqrt(len(y)) * crossval_err / dy + d * np.log(len(y))\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(d, crossval_err, '--k', label='cross-validation')\n",
    "ax.plot(d, training_err, '-k', label='training')\n",
    "ax.plot(d, 0.1 * np.ones(d.shape), ':k')\n",
    "\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(d, BIC_crossval, '--k', label='cross-validation')\n",
    "ax.plot(d, BIC_train, '-k', label='training')\n",
    "\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('BIC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation doesn't tell us how to improve a model:\n",
    "\n",
    "* Get more training data\n",
    "* Use a more/less complicated model\n",
    "* Use more/less regularization\n",
    "* Increase the number of features\n",
    "\n",
    "Learning curve: Plot of training error $\\epsilon_{tr}$ and cross-validation error $\\epsilon_{cv}$ as a function of the size $n$ of the training set.\n",
    "\n",
    "1. Increasing $n$ increases $\\epsilon_{tr}$\n",
    "2. Increasing $n$ decreases $\\epsilon_{cv}$\n",
    "3. $\\epsilon_{tr} \\leq \\epsilon_{cv} \\forall n$\n",
    "4. $n$ large $\\Rightarrow$ Training and cross-validation curves converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips:\n",
    "\n",
    "- Training and cross-validation error have converged $\\Rightarrow$ error dominated by bias (underfitting):\n",
    "\n",
    "    - Increase model complexity\n",
    "    - Add additional features to the data\n",
    "    - Decrease the regularization\n",
    "    \n",
    "- Training error much smaller than cross-validation $\\Rightarrow$ overfit:\n",
    "\n",
    "    - Increase the training set size\n",
    "    - Decrease model complexity\n",
    "    - Increase the regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other cross-validation techniques:\n",
    "\n",
    "Twofold crossvalidation: Divide the sample into $d_{0}$, $d_{1}$: First train in $d_{0}$ and cross-validate in $d_{1}$. Next iteration train in $d_{1}$ and cross-validate in $d_{0}$. Cross-validation error computed as the mean of errors in each fold.\n",
    "\n",
    "$K$-fold cross-validation: Same idea but dividing the data into $K+1$ sets\n",
    "\n",
    "Leave-one-out cross-validation: Each set has just one data point\n",
    "\n",
    "Random subset cross-validation: Training and cross-validation sample are randomly drawn from the data (not every point is guaranteed to be used for training and cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.12 Which regression method should I use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most accurate regression methods? Starting point is linear regression. Adding ridge or LASSO regularization increases accuracy. Adding capability to incorporate measurement errors increases accuracy. Principal component regression increases accuracy.\n",
    "\n",
    "Nonlinear models: Good starting point is linear regression with nonlinear tranformations, sensibility test checking the Gaussianity of errors. Kernel regression and Gaussian processes help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most interpretable regression methods?\n",
    "\n",
    "- Linear methods\n",
    "- LASSO, ridge regression help to identify the most important features. Bayesian formulations help since they make assumptions clear.\n",
    "\n",
    "- Principal component regression (PCR) decreases interpretability\n",
    "- Kernel regression: It depends\n",
    "- Gaussian process: Pretty opaque\n",
    "\n",
    "What are the simplest regression method:\n",
    "\n",
    "- Basic linear\n",
    "- Regression\n",
    "- PCR\n",
    "- Bayesian\n",
    "- Nadaraya-Watson\n",
    "- Basic Gaussian Process\n",
    "\n",
    "What are the most scalable regression methods?\n",
    "\n",
    "- Linear regression\n",
    "- PCR\n",
    "- Kernel regression\n",
    "- Gaussian processes not very scalable (very expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"table.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
